{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75370437-0da6-4965-aa24-2c079d362edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b571e1f4-96bf-4af1-9188-3df7f1f0d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tokenizer from ProtBERT model developed by the Rost lab\n",
    "# Tokenizer for ProtBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1fc6aad-e449-4583-8ed5-6deacaa2f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define BERT-EE (BERT plus entity extraction for the NLS motifs)\n",
    "class BERT_EE(nn.Module):\n",
    "    def __init__(self, input_dim, motif_vocab_size, hidden_dim, num_classes):\n",
    "        super(BERT_EE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "        self.embedding_layer = nn.Embedding(motif_vocab_size, hidden_dim)\n",
    "        self.linear = nn.Linear(input_dim + hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, motif_features):\n",
    "        # ProtBERT embeddings for the protein sequence\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_embeddings = bert_output.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "\n",
    "        # Embedding layer for motif features\n",
    "        motif_embeddings = self.embedding_layer(motif_features)\n",
    "\n",
    "        # Concatenate ProtBERT embeddings with motif embeddings\n",
    "        combined_embeddings = torch.cat((bert_embeddings, motif_embeddings), dim=1)\n",
    "\n",
    "        # Final classification layer\n",
    "        logits = self.linear(combined_embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01028b1b-de3c-425b-8a1f-26d3538ff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"finalized_complete_NLS_sequence_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a37f92c-f2a0-40c3-ab3d-ad6fbce83cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniProt ID</th>\n",
       "      <th>Sequence_full</th>\n",
       "      <th>Name</th>\n",
       "      <th>Begin</th>\n",
       "      <th>End</th>\n",
       "      <th>Sequence_nls</th>\n",
       "      <th>Length</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>ECO code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q14738</td>\n",
       "      <td>MPYKLKKEKEPPKVAKCTAKPSSSGKDGGGENTEEAQPQPQPQPQP...</td>\n",
       "      <td>Serine/threonine-protein phosphatase 2A 56 kDa...</td>\n",
       "      <td>548</td>\n",
       "      <td>565</td>\n",
       "      <td>KRTVETEAVQMLKDIKKE</td>\n",
       "      <td>18</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q13362</td>\n",
       "      <td>MLTCNKAGSRMVVDAANSNGPFQPVVLLHIRDVPPADQEKLFIQKL...</td>\n",
       "      <td>Serine/threonine-protein phosphatase 2A 56 kDa...</td>\n",
       "      <td>416</td>\n",
       "      <td>422</td>\n",
       "      <td>KLKEKLK</td>\n",
       "      <td>7</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9NRA8</td>\n",
       "      <td>MDRRSMGETESGDAFLDLKKPPASKCPHRYTKEELLDIKELPHSKQ...</td>\n",
       "      <td>Eukaryotic translation initiation factor 4E tr...</td>\n",
       "      <td>195</td>\n",
       "      <td>211</td>\n",
       "      <td>RREFGDSKRVFGERRRN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P42684</td>\n",
       "      <td>MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTTETGF...</td>\n",
       "      <td>Abelson tyrosine-protein kinase 2</td>\n",
       "      <td>658</td>\n",
       "      <td>660</td>\n",
       "      <td>KKR</td>\n",
       "      <td>3</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q4JIM5</td>\n",
       "      <td>MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTADAGF...</td>\n",
       "      <td>Abelson tyrosine-protein kinase 2</td>\n",
       "      <td>659</td>\n",
       "      <td>661</td>\n",
       "      <td>KKR</td>\n",
       "      <td>3</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>Q96CK0</td>\n",
       "      <td>MAERALEPEAEAEAEAGAGGEAAAEEGAAGRKARGRPRLTESDRAR...</td>\n",
       "      <td>Zinc finger protein 653</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>PKKPKRKKRRRR</td>\n",
       "      <td>12</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>Q96CK0</td>\n",
       "      <td>MAERALEPEAEAEAEAGAGGEAAAEEGAAGRKARGRPRLTESDRAR...</td>\n",
       "      <td>Zinc finger protein 653</td>\n",
       "      <td>445</td>\n",
       "      <td>451</td>\n",
       "      <td>EPEKRRR</td>\n",
       "      <td>7</td>\n",
       "      <td>Sequence Analysis</td>\n",
       "      <td>ECO:0000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>Q24JY4</td>\n",
       "      <td>MVEKKTSVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...</td>\n",
       "      <td>Zinc finger HIT domain-containing protein 1</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>DNFQDDPHAG</td>\n",
       "      <td>10</td>\n",
       "      <td>By similarity</td>\n",
       "      <td>ECO:0000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>O43257</td>\n",
       "      <td>MVEKKTSVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...</td>\n",
       "      <td>Zinc finger HIT domain-containing protein 1</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>DNFQDDPHAG</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>Q8R331</td>\n",
       "      <td>MVEKKPAVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...</td>\n",
       "      <td>Zinc finger HIT domain-containing protein 1</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>DNFQDDPHAG</td>\n",
       "      <td>10</td>\n",
       "      <td>By similarity</td>\n",
       "      <td>ECO:0000250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1363 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UniProt ID                                      Sequence_full  \\\n",
       "0        Q14738  MPYKLKKEKEPPKVAKCTAKPSSSGKDGGGENTEEAQPQPQPQPQP...   \n",
       "1        Q13362  MLTCNKAGSRMVVDAANSNGPFQPVVLLHIRDVPPADQEKLFIQKL...   \n",
       "2        Q9NRA8  MDRRSMGETESGDAFLDLKKPPASKCPHRYTKEELLDIKELPHSKQ...   \n",
       "3        P42684  MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTTETGF...   \n",
       "4        Q4JIM5  MGQQVGRVGEAPGLQQPQPRGIRGSSAARPSGRRRDPAGRTADAGF...   \n",
       "...         ...                                                ...   \n",
       "1358     Q96CK0  MAERALEPEAEAEAEAGAGGEAAAEEGAAGRKARGRPRLTESDRAR...   \n",
       "1359     Q96CK0  MAERALEPEAEAEAEAGAGGEAAAEEGAAGRKARGRPRLTESDRAR...   \n",
       "1360     Q24JY4  MVEKKTSVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...   \n",
       "1361     O43257  MVEKKTSVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...   \n",
       "1362     Q8R331  MVEKKPAVRSQDPGQRRVLDRAARQRRINRQLEALENDNFQDDPHA...   \n",
       "\n",
       "                                                   Name  Begin  End  \\\n",
       "0     Serine/threonine-protein phosphatase 2A 56 kDa...    548  565   \n",
       "1     Serine/threonine-protein phosphatase 2A 56 kDa...    416  422   \n",
       "2     Eukaryotic translation initiation factor 4E tr...    195  211   \n",
       "3                     Abelson tyrosine-protein kinase 2    658  660   \n",
       "4                     Abelson tyrosine-protein kinase 2    659  661   \n",
       "...                                                 ...    ...  ...   \n",
       "1358                            Zinc finger protein 653    107  118   \n",
       "1359                            Zinc finger protein 653    445  451   \n",
       "1360        Zinc finger HIT domain-containing protein 1     38   47   \n",
       "1361        Zinc finger HIT domain-containing protein 1     38   47   \n",
       "1362        Zinc finger HIT domain-containing protein 1     38   47   \n",
       "\n",
       "            Sequence_nls  Length           Evidence     ECO code  \n",
       "0     KRTVETEAVQMLKDIKKE      18  Sequence Analysis  ECO:0000255  \n",
       "1                KLKEKLK       7  Sequence Analysis  ECO:0000255  \n",
       "2      RREFGDSKRVFGERRRN      17                NaN          NaN  \n",
       "3                    KKR       3  Sequence Analysis  ECO:0000255  \n",
       "4                    KKR       3  Sequence Analysis  ECO:0000255  \n",
       "...                  ...     ...                ...          ...  \n",
       "1358        PKKPKRKKRRRR      12  Sequence Analysis  ECO:0000255  \n",
       "1359             EPEKRRR       7  Sequence Analysis  ECO:0000255  \n",
       "1360          DNFQDDPHAG      10      By similarity  ECO:0000250  \n",
       "1361          DNFQDDPHAG      10                NaN          NaN  \n",
       "1362          DNFQDDPHAG      10      By similarity  ECO:0000250  \n",
       "\n",
       "[1363 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c814359c-34d7-4af3-b63b-de9d93040d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode sequences\n",
    "# Maximum length selected based on original paper describing the ProtBERT model parameters\n",
    "#In order to be stacked, input IDs and attention masks must first be in list format\n",
    "input_ids_list = []\n",
    "attention_masks_list = []\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "for sequence in df['Sequence_nls']:\n",
    "    encoded_seq = tokenizer(sequence, padding='max_length', truncation=True, max_length=2048, return_tensors='pt')\n",
    "    input_ids_list.append(encoded_seq['input_ids'])\n",
    "    attention_masks_list.append(encoded_seq['attention_mask'])\n",
    "\n",
    "# Convert input IDs and attention masks to tensors\n",
    "input_ids = torch.cat(input_ids_list, dim=0)\n",
    "attention_masks = torch.cat(attention_masks_list, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48cf3d02-408c-45a0-9208-a58de8bf52f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize motif features tensor, zeros as default\n",
    "max_sequence_length = max(len(seq) for seq in df['Sequence_nls'])\n",
    "motif_features = torch.zeros(len(df), max_sequence_length)  # Initialize motif features tensor, inflate everything to 2,048 zeros for each tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "383b4d0f-1153-4a2f-b72e-5333fd9e8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize start and end positions tensors, look for maximum length sequence here, populate tensors with zeros\n",
    "max_sequence_length = max(len(seq) for seq in df['Sequence_nls'])\n",
    "start_positions = torch.zeros(len(df), max_sequence_length, dtype=torch.long)\n",
    "end_positions = torch.zeros(len(df), max_sequence_length, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f51d7a50-b4aa-4c00-aa00-027534c37d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate motif features tensor with ones representing the location of NLS sequences\n",
    "for idx, row in df.iterrows():\n",
    "    start = row['Begin'] #row for each protein\n",
    "    end = row['End']\n",
    "    motif_features[idx, start:end+1] = 1  # Set the range of positions corresponding to the motif (NLS in this case) to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14b1edfb-dcef-4e83-a7eb-069499c5aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling (since all these sequences contain NLS sequences, they'll be labelled as such for now\n",
    "labels = torch.ones(len(df), dtype=torch.long)  # Assume all sequences have NLS motifs, so label them as 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "963b2f9d-3b91-44ef-9cbd-01c4e0c5ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.2, random_state=831)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcee324f-89f2-4a67-8af8-5809bc2cfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders for training and test datasets\n",
    "batch_size = 8\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "387c9ad5-7534-4ce7-b7fc-1e74a7a5bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d6b80ebbdf4521a542bc8e4a4b5bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model initialization, optimizer learning rate defined, and cross-entropy loss function\n",
    "# Number of classes set to two for straightforward binary classification\n",
    "model = BERT_EE(input_dim=input_ids.size(1), motif_vocab_size=max_sequence_length*2, hidden_dim=128, num_classes=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ad104-e693-4d8b-8a1f-1d0b9bc47c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_masks, motif_features)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42c513-ba0e-4037-8332-eb2babbccd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids, labels = batch\n",
    "        logits = model(input_ids)\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        test_preds.extend(predicted.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "test_precision = precision_score(test_labels, test_preds)\n",
    "test_recall = recall_score(test_labels, test_preds)\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  Test Precision: {test_precision:.4f}\")\n",
    "print(f\"  Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3eeb3-d7a8-4e8d-961b-02e330b5d563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
